============================================================
WandB Summary Table Generator
============================================================
Entity: llm-topics
Datasets: ['20_newsgroups', 'tweet_topic', 'stackoverflow']
K values: [25, 50, 75, 100]
Required seeds: 5
Metrics: ['cv_wiki', 'llm_rating', 'inverted_rbo', 'purity']
Significance level: 0.05
============================================================

Fetching runs from llm-topics/20_newsgroups...
  Found 131 finished runs
Fetching runs from llm-topics/tweet_topic...
  Found 124 finished runs
Fetching runs from llm-topics/stackoverflow...
  Found 120 finished runs

--- Main Methods Summary ---
  lda on 20_newsgroups: 4 runs
  lda on tweet_topic: 4 runs
  lda on stackoverflow: 4 runs
  prodlda on 20_newsgroups: 4 runs
  prodlda on tweet_topic: 4 runs
  prodlda on stackoverflow: 4 runs
  zeroshot on 20_newsgroups: 4 runs
  zeroshot on tweet_topic: 4 runs
  zeroshot on stackoverflow: 4 runs
  combined on 20_newsgroups: 4 runs
  combined on tweet_topic: 4 runs
  combined on stackoverflow: 4 runs
  etm on 20_newsgroups: 4 runs
  etm on tweet_topic: 4 runs
  etm on stackoverflow: 4 runs
  bertopic on 20_newsgroups: 4 runs
  bertopic on tweet_topic: 4 runs
  bertopic on stackoverflow: 4 runs
  ecrtm on 20_newsgroups: 4 runs
  ecrtm on tweet_topic: 4 runs
  ecrtm on stackoverflow: 4 runs
  fastopic on 20_newsgroups: 4 runs
  fastopic on tweet_topic: 4 runs
  fastopic on stackoverflow: 4 runs
  generative_ERNIE-4.5-0.3B-PT on 20_newsgroups: 4 runs
  generative_ERNIE-4.5-0.3B-PT on tweet_topic: 4 runs
  generative_ERNIE-4.5-0.3B-PT on stackoverflow: 4 runs
  generative_Llama-3.1-8B-Instruct on 20_newsgroups: 4 runs
  generative_Llama-3.1-8B-Instruct on tweet_topic: 4 runs
  generative_Llama-3.1-8B-Instruct on stackoverflow: 4 runs
  generative_Llama-3.2-1B-Instruct on 20_newsgroups: 40 runs
  generative_Llama-3.2-1B-Instruct on tweet_topic: 40 runs
  generative_Llama-3.2-1B-Instruct on stackoverflow: 40 runs

=======================================================================================================================================================
Method                    |              20_newsgroups              |               tweet_topic               |              stackoverflow             
                          |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity
=======================================================================================================================================================
LDA                       |   ~0.341~   ~2.219~     0.977   ~0.301~ |     0.350     1.950     0.992     0.441 |   ~0.354~     2.004     0.977     0.174
ProdLDA                   |     0.351     2.487     0.992     0.356 |     0.355     2.113     0.994     0.533 |     0.352     2.619     0.991     0.265
ZeroShotTM                |     0.354     2.531     0.994     0.397 |     0.356     2.302     0.994     0.573 |     0.363     2.841     0.993     0.307
CombinedTM                |     0.351     2.565     0.993     0.391 |     0.360     2.363     0.988     0.588 |     0.364     2.853     0.986     0.306
ETM                       |   ~0.344~     2.346   ~0.934~     0.360 |     0.351     2.224     0.936     0.552 |   ~0.348~     2.285     0.934     0.151
BERTopic                  |     0.360     2.475     0.992     0.352 |     0.364     2.194     0.996     0.562 |     0.374     2.632     0.996     0.202
ECRTM                     |     0.360   ~2.282~ **0.999**     0.364 |     0.356   ~1.853~ **1.000**   ~0.399~ |     0.373   ~1.946~ **1.000**   ~0.062~
FASTopic                  |     0.358     2.589 **0.999**     0.416 |   ~0.276~     1.956   ~0.626~     0.557 |     0.363     2.304   ~0.687~     0.171
Generative (ERNIE)        | **0.381** **2.863**     0.991     0.520 | **0.392** **2.898**     0.989 **0.781** | **0.397**     2.920     0.986 **0.737**
Generative (Llama-8B)     |     0.364 **2.874**     0.993 **0.559** |     0.384     2.878     0.993 **0.774** |     0.386 **2.956**     0.991     0.700
Generative (Llama-1B)     |     0.375 **2.865**     0.993 **0.559** |     0.383 **2.899**     0.993 **0.781** |     0.393     2.938     0.993     0.689
=======================================================================================================================================================
Legend: **bold** = best or not sig. different from best, ~tilde~ = not sig. different from worst (p >= 0.05)

--- Ablation Experiments Summary ---
  ERNIE-4.5-0.3B-PT/original on 20_newsgroups: 4 runs
  ERNIE-4.5-0.3B-PT/original on tweet_topic: 4 runs
  ERNIE-4.5-0.3B-PT/original on stackoverflow: 4 runs
  ERNIE-4.5-0.3B-PT/bow_target on 20_newsgroups: 8 runs
  ERNIE-4.5-0.3B-PT/bow_target on tweet_topic: 8 runs
  ERNIE-4.5-0.3B-PT/bow_target on stackoverflow: 8 runs
  ERNIE-4.5-0.3B-PT/contextualized_embeddings on 20_newsgroups: 4 runs
  ERNIE-4.5-0.3B-PT/contextualized_embeddings on tweet_topic: 4 runs
  ERNIE-4.5-0.3B-PT/contextualized_embeddings on stackoverflow: 4 runs
  ERNIE-4.5-0.3B-PT/nll_loss on 20_newsgroups: 5 runs
  ERNIE-4.5-0.3B-PT/nll_loss on tweet_topic: 4 runs
  ERNIE-4.5-0.3B-PT/nll_loss on stackoverflow: 4 runs

=======================================================================================================================================================
ABLATION EXPERIMENTS (ERNIE-4.5-0.3B-PT)
=======================================================================================================================================================
Ablation                  |              20_newsgroups              |               tweet_topic               |              stackoverflow             
                          |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity
=======================================================================================================================================================
Original (Ours)           |     0.381 **2.863** **0.991** **0.520** |     0.392 **2.898**     0.989 **0.781** |     0.397 **2.920**     0.986 **0.737**
BoW Target                |   ~0.339~   ~2.480~ **0.991**   ~0.432~ |   ~0.357~   ~2.160~ **0.994**   ~0.624~ |   ~0.369~   ~2.769~ **0.993**   ~0.374~
Contextualized Embeddings |     0.381 **2.855**     0.988 **0.513** |     0.396 **2.880**     0.985     0.721 |     0.396     2.903     0.979     0.513
NLL Loss                  | **0.407**     2.778   ~0.969~     0.468 | **0.406**     2.762   ~0.965~     0.762 | **0.419**     2.834   ~0.976~     0.716
=======================================================================================================================================================
Legend: **bold** = best or not sig. different from best, ~tilde~ = not sig. different from worst (p >= 0.05)
  Llama-3.1-8B-Instruct/original on 20_newsgroups: 4 runs
  Llama-3.1-8B-Instruct/original on tweet_topic: 4 runs
  Llama-3.1-8B-Instruct/original on stackoverflow: 4 runs
  Llama-3.1-8B-Instruct/bow_target on 20_newsgroups: 8 runs
  Llama-3.1-8B-Instruct/bow_target on tweet_topic: 6 runs
  Llama-3.1-8B-Instruct/bow_target on stackoverflow: 4 runs
  Llama-3.1-8B-Instruct/contextualized_embeddings on 20_newsgroups: 4 runs
  Llama-3.1-8B-Instruct/contextualized_embeddings on tweet_topic: 4 runs
  Llama-3.1-8B-Instruct/contextualized_embeddings on stackoverflow: 4 runs
  Llama-3.1-8B-Instruct/nll_loss on 20_newsgroups: 4 runs
  Llama-3.1-8B-Instruct/nll_loss on tweet_topic: 4 runs
  Llama-3.1-8B-Instruct/nll_loss on stackoverflow: 4 runs

=======================================================================================================================================================
ABLATION EXPERIMENTS (Llama-3.1-8B-Instruct)
=======================================================================================================================================================
Ablation                  |              20_newsgroups              |               tweet_topic               |              stackoverflow             
                          |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity
=======================================================================================================================================================
Original (Ours)           |     0.364 **2.874** **0.993** **0.559** |     0.384     2.878     0.993 **0.774** |     0.386 **2.956** **0.991** **0.700**
BoW Target                |   ~0.339~   ~2.471~     0.991   ~0.452~ |   ~0.358~   ~2.142~ **0.994**   ~0.599~ |   ~0.364~   ~2.741~ **0.993**   ~0.346~
Contextualized Embeddings |     0.366 **2.859**     0.990 **0.552** |     0.382     2.855     0.989     0.715 |     0.384     2.942     0.986     0.482
NLL Loss                  | **0.396**     2.769   ~0.973~     0.500 | **0.389** **2.910**   ~0.958~ **0.769** | **0.398**     2.918   ~0.979~ **0.701**
=======================================================================================================================================================
Legend: **bold** = best or not sig. different from best, ~tilde~ = not sig. different from worst (p >= 0.05)
  Llama-3.2-1B-Instruct/original on 20_newsgroups: 40 runs
  Llama-3.2-1B-Instruct/original on tweet_topic: 40 runs
  Llama-3.2-1B-Instruct/original on stackoverflow: 40 runs
  Llama-3.2-1B-Instruct/bow_target on 20_newsgroups: 4 runs
  Llama-3.2-1B-Instruct/bow_target on tweet_topic: 2 runs
  Llama-3.2-1B-Instruct/contextualized_embeddings on 20_newsgroups: 4 runs
  Llama-3.2-1B-Instruct/contextualized_embeddings on tweet_topic: 4 runs
  Llama-3.2-1B-Instruct/contextualized_embeddings on stackoverflow: 4 runs
  Llama-3.2-1B-Instruct/nll_loss on 20_newsgroups: 4 runs
  Llama-3.2-1B-Instruct/nll_loss on tweet_topic: 2 runs
  Llama-3.2-1B-Instruct/nll_loss on stackoverflow: 2 runs

=======================================================================================================================================================
ABLATION EXPERIMENTS (Llama-3.2-1B-Instruct)
=======================================================================================================================================================
Ablation                  |              20_newsgroups              |               tweet_topic               |              stackoverflow             
                          |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity |        CV       LLM     I-RBO    Purity
=======================================================================================================================================================
Original (Ours)           |     0.375     2.865 **0.993** **0.559** |     0.383 **2.899**     0.993 **0.781** |   ~0.393~ **2.938** **0.993** **0.689**
BoW Target                |   ~0.341~   ~2.477~     0.991   ~0.443~ |   ~0.355~   ~2.228~ **0.996**   ~0.595~ |         -         -         -         -
Contextualized Embeddings |     0.381 **2.890**     0.989     0.549 |     0.390 **2.910**     0.989     0.723 | **0.402**     2.913   ~0.985~   ~0.468~
NLL Loss                  | **0.398**     2.748   ~0.968~     0.503 | **0.405**     2.858   ~0.963~     0.748 | **0.400**   ~2.862~   ~0.982~     0.623
=======================================================================================================================================================
Legend: **bold** = best or not sig. different from best, ~tilde~ = not sig. different from worst (p >= 0.05)

--- Retrieval Evaluation Summary ---
  lda on 20_newsgroups: 4 runs (retrieval)
  lda on tweet_topic: 4 runs (retrieval)
  lda on stackoverflow: 4 runs (retrieval)
  prodlda on 20_newsgroups: 4 runs (retrieval)
  prodlda on tweet_topic: 4 runs (retrieval)
  prodlda on stackoverflow: 4 runs (retrieval)
  zeroshot on 20_newsgroups: 4 runs (retrieval)
  zeroshot on tweet_topic: 4 runs (retrieval)
  zeroshot on stackoverflow: 4 runs (retrieval)
  combined on 20_newsgroups: 4 runs (retrieval)
  combined on tweet_topic: 4 runs (retrieval)
  combined on stackoverflow: 4 runs (retrieval)
  etm on 20_newsgroups: 4 runs (retrieval)
  etm on tweet_topic: 4 runs (retrieval)
  etm on stackoverflow: 4 runs (retrieval)
  bertopic on 20_newsgroups: 4 runs (retrieval)
  bertopic on tweet_topic: 4 runs (retrieval)
  bertopic on stackoverflow: 4 runs (retrieval)
  ecrtm on 20_newsgroups: 4 runs (retrieval)
  ecrtm on tweet_topic: 4 runs (retrieval)
  ecrtm on stackoverflow: 4 runs (retrieval)
  fastopic on 20_newsgroups: 4 runs (retrieval)
  fastopic on tweet_topic: 4 runs (retrieval)
  fastopic on stackoverflow: 4 runs (retrieval)
  generative_ERNIE-4.5-0.3B-PT on 20_newsgroups: 4 runs (retrieval)
  generative_ERNIE-4.5-0.3B-PT on tweet_topic: 4 runs (retrieval)
  generative_ERNIE-4.5-0.3B-PT on stackoverflow: 4 runs (retrieval)
  generative_Llama-3.1-8B-Instruct on 20_newsgroups: 4 runs (retrieval)
  generative_Llama-3.1-8B-Instruct on tweet_topic: 4 runs (retrieval)
  generative_Llama-3.1-8B-Instruct on stackoverflow: 4 runs (retrieval)
  generative_Llama-3.2-1B-Instruct on 20_newsgroups: 4 runs (retrieval)
  generative_Llama-3.2-1B-Instruct on tweet_topic: 4 runs (retrieval)
  generative_Llama-3.2-1B-Instruct on stackoverflow: 4 runs (retrieval)
  bow on 20_newsgroups: 3 runs (baseline)
  bow on tweet_topic: 3 runs (baseline)
  bow on stackoverflow: 3 runs (baseline)
  llm_targets_ERNIE-4.5-0.3B-PT on 20_newsgroups: 1 runs (baseline)
  llm_targets_ERNIE-4.5-0.3B-PT on tweet_topic: 1 runs (baseline)
  llm_targets_ERNIE-4.5-0.3B-PT on stackoverflow: 1 runs (baseline)
  llm_targets_Llama-3.1-8B-Instruct on 20_newsgroups: 1 runs (baseline)
  llm_targets_Llama-3.1-8B-Instruct on tweet_topic: 1 runs (baseline)
  llm_targets_Llama-3.1-8B-Instruct on stackoverflow: 1 runs (baseline)
  llm_targets_Llama-3.2-1B-Instruct on 20_newsgroups: 1 runs (baseline)
  llm_targets_Llama-3.2-1B-Instruct on tweet_topic: 1 runs (baseline)
  llm_targets_Llama-3.2-1B-Instruct on stackoverflow: 1 runs (baseline)

=========================================================================================================================
RETRIEVAL EVALUATION (Precision@K)
=========================================================================================================================
Method                    |         20_newsgroups         |          tweet_topic          |         stackoverflow        
                          |       P@1       P@5      P@10 |       P@1       P@5      P@10 |       P@1       P@5      P@10
=========================================================================================================================
LDA                       |   ~0.278~   ~0.244~   ~0.231~ |     0.892     0.674   ~0.500~ |   ~0.121~   ~0.113~   ~0.109~
ProdLDA                   |     0.369     0.346     0.334 |     0.951     0.767     0.619 |     0.263     0.242     0.231
ZeroShotTM                |     0.404     0.384     0.373 |     0.942     0.776     0.647 |     0.302     0.280     0.268
CombinedTM                |     0.381     0.366     0.357 |     0.894     0.744     0.637 |     0.274     0.260     0.251
ETM                       |     0.329     0.312     0.305 |     0.963     0.786     0.637 |     0.156     0.135     0.125
BERTopic                  |     0.435     0.413     0.402 |   ~0.585~   ~0.546~   ~0.519~ |     0.322     0.306     0.296
ECRTM                     |     0.375     0.341     0.329 | **0.937**     0.712   ~0.530~ |     0.138   ~0.114~   ~0.101~
FASTopic                  |     0.461     0.421     0.406 | **0.985**     0.811     0.667 |     0.211     0.189     0.178
Generative (ERNIE)        |     0.528     0.509     0.499 | **0.973** **0.900**     0.844 |     0.767     0.754     0.746
Generative (Llama-8B)     | **0.598** **0.577** **0.565** | **0.976** **0.909** **0.854** | **0.802** **0.784** **0.772**
Generative (Llama-1B)     |     0.571     0.553     0.543 | **0.977** **0.904**     0.846 |     0.769     0.752     0.741
-------------------------------------------------------------------------------------------------------------------------
BoW (Baseline)            |     0.376     0.311     0.288 |     0.898     0.716     0.573 |     0.265     0.229     0.213
LLM Targets (ERNIE)       |     0.605     0.551     0.524 |     1.000     0.923     0.853 |     0.786     0.754     0.735
LLM Targets (Llama-8B)    |     0.689     0.626     0.594 |     1.000     0.929     0.867 |     0.800     0.765     0.745
LLM Targets (Llama-1B)    |     0.665     0.609     0.584 |     1.000     0.922     0.856 |     0.754     0.713     0.688
=========================================================================================================================
Legend: **bold** = best or not sig. different from best, ~tilde~ = not sig. different from worst (p >= 0.05)

========================================================================================================================
RETRIEVAL EVALUATION BY K (with significance testing)
========================================================================================================================

--- K = 25 ---
-------------------------------------------------------------------------------------------------------------------------
Method                    |         20_newsgroups         |          tweet_topic          |         stackoverflow        
                          |       P@1       P@5      P@10 |       P@1       P@5      P@10 |       P@1       P@5      P@10
-------------------------------------------------------------------------------------------------------------------------
LDA                       |   ~0.267~   ~0.247~   ~0.238~ |     0.827     0.625   ~0.470~ |   ~0.094~   ~0.085~   ~0.081~
ProdLDA                   |     0.330     0.315     0.306 |     0.905     0.732     0.599 |     0.241     0.224     0.214
ZeroShotTM                |     0.363     0.349     0.341 |     0.880     0.732     0.623 |     0.271     0.254     0.246
CombinedTM                |     0.345     0.334     0.328 |     0.792     0.673     0.599 |     0.247     0.236     0.229
ETM                       |     0.322     0.298     0.290 |     0.981     0.797     0.639 |     0.160     0.137     0.127
BERTopic                  |     0.414     0.390     0.380 |   ~0.519~   ~0.473~   ~0.431~ |     0.313     0.294     0.283
ECRTM                     |     0.347     0.320     0.309 | **0.984**     0.745     0.547 |     0.118     0.094   ~0.084~
FASTopic                  |     0.424     0.391     0.378 | **0.985**     0.792     0.635 |     0.165     0.146     0.136
Generative (ERNIE)        |     0.481     0.471     0.465 |     0.913     0.856     0.821 | **0.735** **0.725** **0.718**
Generative (Llama-8B)     | **0.537** **0.526** **0.519** |     0.931 **0.874** **0.834** | **0.735** **0.720** **0.711**
Generative (Llama-1B)     | **0.528** **0.516** **0.510** |     0.927     0.867     0.828 | **0.723**     0.711     0.703
-------------------------------------------------------------------------------------------------------------------------
BoW (Baseline)            |     0.376     0.311     0.288 |     0.898     0.716     0.573 |     0.265     0.229     0.213
LLM Targets (ERNIE)       |     0.605     0.551     0.524 |     1.000     0.923     0.853 |     0.786     0.754     0.735
LLM Targets (Llama-8B)    |     0.689     0.626     0.594 |     1.000     0.929     0.867 |     0.800     0.765     0.745
LLM Targets (Llama-1B)    |     0.665     0.609     0.584 |     1.000     0.922     0.856 |     0.754     0.713     0.688
-------------------------------------------------------------------------------------------------------------------------

--- K = 50 ---
-------------------------------------------------------------------------------------------------------------------------
Method                    |         20_newsgroups         |          tweet_topic          |         stackoverflow        
                          |       P@1       P@5      P@10 |       P@1       P@5      P@10 |       P@1       P@5      P@10
-------------------------------------------------------------------------------------------------------------------------
LDA                       |   ~0.284~   ~0.251~   ~0.237~ |     0.893     0.671   ~0.497~ |   ~0.116~   ~0.106~     0.102
ProdLDA                   |     0.368     0.346     0.335 |     0.960     0.774     0.623 |     0.269     0.246     0.234
ZeroShotTM                |     0.403     0.385     0.375 |     0.950     0.779     0.649 |     0.301     0.279     0.268
CombinedTM                |     0.384     0.367     0.358 |     0.901     0.746     0.638 |     0.277     0.263     0.255
ETM                       |     0.337     0.320     0.313 |     0.969     0.790     0.642 |     0.160     0.138     0.127
BERTopic                  |     0.434     0.412     0.399 |   ~0.601~   ~0.559~     0.535 |     0.320     0.304     0.295
ECRTM                     |     0.379     0.348     0.336 |     0.985     0.745     0.546 |     0.130   ~0.107~   ~0.097~
FASTopic                  |     0.461     0.421     0.406 |     0.985     0.810     0.665 |     0.206     0.184     0.174
Generative (ERNIE)        |     0.525     0.507     0.497 | **0.986**     0.908     0.847 |     0.769     0.756     0.748
Generative (Llama-8B)     | **0.602** **0.583** **0.571** |     0.985 **0.914** **0.856** | **0.819** **0.802** **0.791**
Generative (Llama-1B)     |     0.573     0.554     0.544 | **0.987**     0.910     0.848 |     0.776     0.758     0.747
-------------------------------------------------------------------------------------------------------------------------
BoW (Baseline)            |     0.376     0.311     0.288 |     0.898     0.716     0.573 |     0.265     0.229     0.213
LLM Targets (ERNIE)       |     0.605     0.551     0.524 |     1.000     0.923     0.853 |     0.786     0.754     0.735
LLM Targets (Llama-8B)    |     0.689     0.626     0.594 |     1.000     0.929     0.867 |     0.800     0.765     0.745
LLM Targets (Llama-1B)    |     0.665     0.609     0.584 |     1.000     0.922     0.856 |     0.754     0.713     0.688
-------------------------------------------------------------------------------------------------------------------------

--- K = 75 ---
-------------------------------------------------------------------------------------------------------------------------
Method                    |         20_newsgroups         |          tweet_topic          |         stackoverflow        
                          |       P@1       P@5      P@10 |       P@1       P@5      P@10 |       P@1       P@5      P@10
-------------------------------------------------------------------------------------------------------------------------
LDA                       |   ~0.282~   ~0.244~   ~0.227~ |     0.922     0.696   ~0.513~ |   ~0.134~   ~0.128~     0.124
ProdLDA                   |     0.383     0.357     0.343 |     0.968     0.779     0.625 |     0.274     0.251     0.239
ZeroShotTM                |     0.420     0.396     0.384 |     0.965     0.792     0.655 |     0.315     0.289     0.277
CombinedTM                |     0.397     0.378     0.369 |     0.935     0.771     0.651 |     0.284     0.268     0.259
ETM                       |     0.337     0.320     0.314 |     0.954     0.782     0.637 |     0.156     0.135     0.126
BERTopic                  |     0.445     0.424     0.413 |   ~0.633~   ~0.572~     0.551 |     0.324     0.310     0.301
ECRTM                     |     0.387     0.351     0.338 |     0.984     0.751     0.551 |     0.148   ~0.123~   ~0.109~
FASTopic                  |     0.475     0.432     0.416 |     0.985     0.819     0.682 |     0.230     0.208     0.196
Generative (ERNIE)        |     0.547     0.524     0.513 | **0.996**     0.918     0.854 |     0.778     0.764     0.755
Generative (Llama-8B)     | **0.623** **0.596** **0.583** |     0.994 **0.923** **0.861** | **0.826** **0.806** **0.793**
Generative (Llama-1B)     |     0.587     0.566     0.555 | **0.996**     0.917     0.852 |     0.790     0.771     0.760
-------------------------------------------------------------------------------------------------------------------------
BoW (Baseline)            |     0.376     0.311     0.288 |     0.898     0.716     0.573 |     0.265     0.229     0.213
LLM Targets (ERNIE)       |     0.605     0.551     0.524 |     1.000     0.923     0.853 |     0.786     0.754     0.735
LLM Targets (Llama-8B)    |     0.689     0.626     0.594 |     1.000     0.929     0.867 |     0.800     0.765     0.745
LLM Targets (Llama-1B)    |     0.665     0.609     0.584 |     1.000     0.922     0.856 |     0.754     0.713     0.688
-------------------------------------------------------------------------------------------------------------------------

--- K = 100 ---
-------------------------------------------------------------------------------------------------------------------------
Method                    |         20_newsgroups         |          tweet_topic          |         stackoverflow        
                          |       P@1       P@5      P@10 |       P@1       P@5      P@10 |       P@1       P@5      P@10
-------------------------------------------------------------------------------------------------------------------------
LDA                       |   ~0.279~   ~0.236~   ~0.220~ |     0.926     0.705   ~0.521~ |   ~0.141~   ~0.133~     0.129
ProdLDA                   |     0.395     0.366     0.353 |     0.970     0.782   ~0.629~ |     0.270     0.248     0.236
ZeroShotTM                |     0.430     0.404     0.391 |     0.972     0.799   ~0.660~ |     0.320     0.296     0.283
CombinedTM                |     0.401     0.383     0.373 |     0.948     0.785   ~0.660~ |     0.288     0.271     0.262
ETM                       |     0.322     0.309     0.302 |     0.947     0.775   ~0.632~ |   ~0.148~   ~0.130~   ~0.120~
BERTopic                  |     0.447     0.427     0.416 |   ~0.585~   ~0.582~   ~0.558~ |     0.330     0.316     0.307
ECRTM                     |     0.386     0.347     0.333 | **~0.793~** **~0.606~**   ~0.475~ |     0.159   ~0.131~   ~0.115~
FASTopic                  |     0.484     0.441     0.423 |     0.986     0.823   ~0.687~ |     0.242     0.219     0.206
Generative (ERNIE)        |     0.560     0.533     0.520 |     0.998     0.919     0.854 |     0.785     0.770     0.762
Generative (Llama-8B)     | **0.630** **0.603** **0.589** |     0.996 **0.925** **0.864** | **0.828** **0.807** **0.794**
Generative (Llama-1B)     |     0.596     0.575     0.563 | **0.998**     0.921     0.856 |     0.789     0.768     0.755
-------------------------------------------------------------------------------------------------------------------------
BoW (Baseline)            |     0.376     0.311     0.288 |     0.898     0.716     0.573 |     0.265     0.229     0.213
LLM Targets (ERNIE)       |     0.605     0.551     0.524 |     1.000     0.923     0.853 |     0.786     0.754     0.735
LLM Targets (Llama-8B)    |     0.689     0.626     0.594 |     1.000     0.929     0.867 |     0.800     0.765     0.745
LLM Targets (Llama-1B)    |     0.665     0.609     0.584 |     1.000     0.922     0.856 |     0.754     0.713     0.688
-------------------------------------------------------------------------------------------------------------------------

Legend: **bold** = best or not sig. different from best, ~tilde~ = not sig. different from worst (p >= 0.05)
