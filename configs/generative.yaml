# Generative Topic Model (Ours)
# Uses LLM-generated logits for topic modeling

model: generative

# Training hyperparameters
num_epochs: 100
batch_size: 64
lr: 2e-3

# Architecture
hidden_size: 200
num_hidden_layers: 2
activation: softplus
solver: adam

# Generative model specific
loss_weight: 1.0
sparsity_ratio: 1.0
loss_type: KL
temperature: 3.0

